{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spidermenDFAttempt3-pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAA1emm7HbAi",
        "colab_type": "code",
        "outputId": "96eedd6e-ecb7-490c-d569-615979cd556e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OZ0NzJ_HhKf",
        "colab_type": "code",
        "outputId": "32fd6d26-fe39-4820-9ec9-b83792903e24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd gdrive/'My Drive'/spidermen"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/spidermen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6eAeuT-yAhL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch.utils.data\n",
        "from torch.nn import functional as F\n",
        "import cv2\n",
        "import math\n",
        "import torch\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.functional import pad\n",
        "from torch.nn.modules import Module\n",
        "from torch.nn.modules.utils import _single, _pair, _triple\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "import os\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "### Source: The function below is the  umeyama function from scikit-image library\n",
        "### specifically the scikit-image/skimage/transform/_geometric.py file\n",
        "###  and is used to warp the inputted image while \n",
        "### training the autoencoder and decoder\n",
        "\n",
        "def umeyama(src, dst, estimate_scale):\n",
        "    \"\"\"Estimate N-D similarity transformation with or without scaling.\n",
        "    Parameters\n",
        "    ----------\n",
        "    src : (M, N) array\n",
        "        Source coordinates.\n",
        "    dst : (M, N) array\n",
        "        Destination coordinates.\n",
        "    estimate_scale : bool\n",
        "        Whether to estimate scaling factor.\n",
        "    Returns\n",
        "    -------\n",
        "    T : (N + 1, N + 1)\n",
        "        The homogeneous similarity transformation matrix. The matrix contains\n",
        "        NaN values only if the problem is not well-conditioned.\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] \"Least-squares estimation of transformation parameters between two\n",
        "            point patterns\", Shinji Umeyama, PAMI 1991, DOI: 10.1109/34.88573\n",
        "    \"\"\"\n",
        "\n",
        "    num = src.shape[0]\n",
        "    dim = src.shape[1]\n",
        "\n",
        "    # Compute mean of src and dst.\n",
        "    src_mean = src.mean(axis=0)\n",
        "    dst_mean = dst.mean(axis=0)\n",
        "\n",
        "    # Subtract mean from src and dst.\n",
        "    src_demean = src - src_mean\n",
        "    dst_demean = dst - dst_mean\n",
        "\n",
        "    # Eq. (38).\n",
        "    A = np.dot(dst_demean.T, src_demean) / num\n",
        "\n",
        "    # Eq. (39).\n",
        "    d = np.ones((dim,), dtype=np.double)\n",
        "    if np.linalg.det(A) < 0:\n",
        "        d[dim - 1] = -1\n",
        "\n",
        "    T = np.eye(dim + 1, dtype=np.double)\n",
        "\n",
        "    U, S, V = np.linalg.svd(A)\n",
        "\n",
        "    # Eq. (40) and (43).\n",
        "    rank = np.linalg.matrix_rank(A)\n",
        "    if rank == 0:\n",
        "        return np.nan * T\n",
        "    elif rank == dim - 1:\n",
        "        if np.linalg.det(U) * np.linalg.det(V) > 0:\n",
        "            T[:dim, :dim] = np.dot(U, V)\n",
        "        else:\n",
        "            s = d[dim - 1]\n",
        "            d[dim - 1] = -1\n",
        "            T[:dim, :dim] = np.dot(U, np.dot(np.diag(d), V))\n",
        "            d[dim - 1] = s\n",
        "    else:\n",
        "        T[:dim, :dim] = np.dot(U, np.dot(np.diag(d), V.T))\n",
        "\n",
        "    if estimate_scale:\n",
        "        # Eq. (41) and (42).\n",
        "        scale = 1.0 / src_demean.var(axis=0).sum() * np.dot(S, d)\n",
        "    else:\n",
        "        scale = 1.0\n",
        "\n",
        "    T[:dim, dim] = dst_mean - scale * np.dot(T[:dim, :dim], src_mean.T)\n",
        "    T[:dim, :dim] *= scale\n",
        "\n",
        "    return T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc4Q5s3Qzq1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random_transform_args = {\n",
        "    'rotation_range': 10,\n",
        "    'zoom_range': 0.05,\n",
        "    'shift_range': 0.05,\n",
        "    'random_flip': 0.4,\n",
        "}\n",
        "\n",
        "\n",
        "### Uses set values to rotate, zoom and shift the images within the inputted \n",
        "### array with a chance of flipping the image. \n",
        "### This is used on the image of the OG dataset while creating the the original\n",
        "### dataset https://github.com/Oldpan/Faceswap-Deepfake-Pytorch\n",
        "\n",
        "def random_transform(image, rotation_range, zoom_range, shift_range, random_flip):\n",
        "    h, w = image.shape[0:2]\n",
        "    rotation = np.random.uniform(-rotation_range, rotation_range)\n",
        "    scale = np.random.uniform(1 - zoom_range, 1 + zoom_range)\n",
        "    transformationOnX = np.random.uniform(-shift_range, shift_range) * w\n",
        "    ty = np.random.uniform(-shift_range, shift_range) * h\n",
        "    mat = cv2.getRotationMatrix2D((w // 2, h // 2), rotation, scale)\n",
        "    mat[:, 2] += (transformationOnX, ty)\n",
        "    result = cv2.warpAffine(image, mat, (w, h), borderMode=cv2.BORDER_REPLICATE)\n",
        "    #randomly flips the image\n",
        "    if np.random.random() < random_flip:\n",
        "        result = result[:, ::-1]\n",
        "    return result\n",
        "\n",
        "\n",
        "# This function is used to generate a pair of random warped images from the \n",
        "# face image which is used to train the autoencoder and decoders. This \n",
        "# was sourced from https://github.com/Oldpan/Faceswap-Deepfake-Pytorch\n",
        "\n",
        "def random_warp(image):\n",
        "    assert image.shape == (256, 256, 3)\n",
        "    range_ = np.linspace(128 - 80, 128 + 80, 5)\n",
        "    mapx = np.broadcast_to(range_, (5, 5))\n",
        "    mapy = mapx.T\n",
        "\n",
        "    mapx = mapx + np.random.normal(size=(5, 5), scale=5)\n",
        "    mapy = mapy + np.random.normal(size=(5, 5), scale=5)\n",
        "\n",
        "    interp_mapx = cv2.resize(mapx, (80, 80))[8:72, 8:72].astype('float32')\n",
        "    interp_mapy = cv2.resize(mapy, (80, 80))[8:72, 8:72].astype('float32')\n",
        "\n",
        "    warped_image = cv2.remap(image, interp_mapx, interp_mapy, cv2.INTER_LINEAR)\n",
        "\n",
        "    src_points = np.stack([mapx.ravel(), mapy.ravel()], axis=-1)\n",
        "    dst_points = np.mgrid[0:65:16, 0:65:16].T.reshape(-1, 2)\n",
        "    mat = umeyama(src_points, dst_points, True)[0:2]\n",
        "\n",
        "    target_image = cv2.warpAffine(image, mat, (64, 64))\n",
        "\n",
        "    return warped_image, target_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mn3byIolz4zD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## creates training batches to be used in the training loop of the autoencoders\n",
        "## Uses Oldpan's random_warp and random_transform functions to\n",
        "## warp and transform images prior to inputting into autoencoder.\n",
        "\n",
        "## InputType: List[images], int\n",
        "\"\"\"\n",
        "Input Description:\n",
        "images: These would be the list of images loaded into memory to be \n",
        "used for training\n",
        "\n",
        "batch_size: The batch size for each step of training\n",
        "\"\"\"\n",
        "## Output: List[images], List[images]\n",
        "def generateTrainingData(images, batch_size):\n",
        "    ### selects a random batch_size number of images from the dataset\n",
        "    ix = np.random.randint(len(images), size=batch_size)\n",
        "    \n",
        "    for i in range(len(ix)):\n",
        "        img = images[ix[i]]\n",
        "        img = random_transform(img, **random_transform_args)\n",
        "        warpedImage, TargetImage = random_warp(img)\n",
        "        \n",
        "        if i == 0:\n",
        "            warpedImages = np.empty((batch_size,) + warpedImage.shape, warpedImage.dtype)\n",
        "            targetImages = np.empty((batch_size,) + TargetImage.shape, warpedImage.dtype)\n",
        "\n",
        "        warpedImages[i] = warpedImage\n",
        "        targetImages[i] = TargetImage\n",
        "\n",
        "    return warpedImages, targetImages"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xZweMvm0BTq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### This is PyTorch source code for the ConvNd module which is being\n",
        "### redefined as the Conv2d forward method is overriden\n",
        "\n",
        "class _ConvNd(Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride,\n",
        "                 padding, dilation, transposed, output_padding, groups, bias):\n",
        "        super(_ConvNd, self).__init__()\n",
        "        if in_channels % groups != 0:\n",
        "            raise ValueError('in_channels must be divisible by groups')\n",
        "        if out_channels % groups != 0:\n",
        "            raise ValueError('out_channels must be divisible by groups')\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.dilation = dilation\n",
        "        self.transposed = transposed\n",
        "        self.output_padding = output_padding\n",
        "        self.groups = groups\n",
        "        if transposed:\n",
        "            self.weight = Parameter(torch.Tensor(\n",
        "                in_channels, out_channels // groups, *kernel_size))\n",
        "        else:\n",
        "            self.weight = Parameter(torch.Tensor(\n",
        "                out_channels, in_channels // groups, *kernel_size))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_channels))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        n = self.in_channels\n",
        "        for k in self.kernel_size:\n",
        "            n *= k\n",
        "        stdv = 1. / math.sqrt(n)\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "### PyTorch source code, with the forward method being overridden to deal with \n",
        "### padding = 'same' option\n",
        "class Conv2d(_ConvNd):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
        "                 padding=0, dilation=1, groups=1, bias=True):\n",
        "        kernel_size = _pair(kernel_size)\n",
        "        stride = _pair(stride)\n",
        "        padding = _pair(padding)\n",
        "        dilation = _pair(dilation)\n",
        "        super(Conv2d, self).__init__(\n",
        "            in_channels, out_channels, kernel_size, stride, padding, dilation,\n",
        "            False, _pair(0), groups, bias)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return conv2d_same_padding(input, self.weight, self.bias, self.stride,\n",
        "                        self.padding, self.dilation, self.groups)\n",
        "\n",
        "\n",
        "### A custom  conv2d method written as Torch doesn't have \"padding='same'\" option\n",
        "### as seen in TensorFlow Keras' Conv2D layers. The code below has been hacked together from \n",
        "### TensorFlow Keras source code \n",
        "### and sourced from https://github.com/hanqingguo/deepfake-pytorch\n",
        "### The override is created so that the output is the same length as the input\n",
        "### mimicking the \"padding = same\" option in TensorFlow Keras.\n",
        "def conv2d_same_padding(input, weight, bias=None, stride=1, padding=1, dilation=1, groups=1):\n",
        "\n",
        "    input_rows = input.size(2)\n",
        "    filter_rows = weight.size(2)\n",
        "    effective_filter_size_rows = (filter_rows - 1) * dilation[0] + 1\n",
        "    out_rows = (input_rows + stride[0] - 1) // stride[0]\n",
        "    padding_needed = max(0, (out_rows - 1) * stride[0] + effective_filter_size_rows -\n",
        "                  input_rows)\n",
        "    padding_rows = max(0, (out_rows - 1) * stride[0] +\n",
        "                        (filter_rows - 1) * dilation[0] + 1 - input_rows)\n",
        "    rows_odd = (padding_rows % 2 != 0)\n",
        "    padding_cols = max(0, (out_rows - 1) * stride[0] +\n",
        "                        (filter_rows - 1) * dilation[0] + 1 - input_rows)\n",
        "    cols_odd = (padding_rows % 2 != 0)\n",
        "\n",
        "    if rows_odd or cols_odd:\n",
        "        input = pad(input, [0, int(cols_odd), 0, int(rows_odd)])\n",
        "\n",
        "    return F.conv2d(input, weight, bias, stride,\n",
        "                  padding=(padding_rows // 2, padding_cols // 2),\n",
        "                  dilation=dilation, groups=groups)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t__Ch_ev0Jyp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_image_paths(dir):\n",
        "    return [x.path for x in os.scandir(dir) if x.name.endswith(\".jpg\") or x.name.endswith(\".png\")\n",
        "            or x.name.endswith(\".JPG\")]\n",
        "\n",
        "def normalize_images(imageList):\n",
        "  imageList = imageList / 255.0\n",
        "  return imageList\n",
        "\n",
        "def load_images(img_path):\n",
        "    imageList = (cv2.resize(cv2.imread(fn), (256, 256)) for fn in img_path)\n",
        "\n",
        "    for i, image in enumerate(imageList):\n",
        "        if i == 0:\n",
        "            finalImageList = np.empty((len(img_path),) + image.shape, dtype=image.dtype)\n",
        "        finalImageList[i] = image\n",
        "    return finalImageList\n",
        "\n",
        "def get_transpose_axes(n):\n",
        "    if n % 2 == 0:\n",
        "        y_axes = list(range(1, n - 1, 2))\n",
        "        x_axes = list(range(0, n - 1, 2))\n",
        "    else:\n",
        "        y_axes = list(range(0, n - 1, 2))\n",
        "        x_axes = list(range(1, n - 1, 2))\n",
        "    return y_axes, x_axes, [n - 1]\n",
        "\n",
        "\n",
        "def stack_images(images):\n",
        "    images_shape = np.array(images.shape)\n",
        "    new_axes = get_transpose_axes(len(images_shape))\n",
        "    new_shape = [np.prod(images_shape[x]) for x in new_axes]\n",
        "    return np.transpose(\n",
        "        images,\n",
        "        axes=np.concatenate(new_axes)\n",
        "    ).reshape(new_shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYHhgmmr0Zkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "\n",
        "def toTensor(img):\n",
        "    img = torch.from_numpy(img.transpose((0, 3, 1, 2)))\n",
        "    return img\n",
        "\n",
        "\n",
        "def var_to_np(img_var):\n",
        "    return img_var.data.cpu().numpy()\n",
        "\n",
        "### A simple custom layer within the NN which flattens the tensor being \n",
        "### passed through it\n",
        "### sourced from https://github.com/hanqingguo/deepfake-pytorch\n",
        "class Flatten(nn.Module):\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = input.view(input.size(0), -1)\n",
        "        return output\n",
        "\n",
        "### A simple custom layer within the NN which reshapes the tensor being \n",
        "### passed through it\n",
        "### sourced from https://github.com/hanqingguo/deepfake-pytorch\n",
        "class Reshape(nn.Module):\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = input.view(-1, 1024, 4, 4)  # channel * 4 * 4\n",
        "\n",
        "        return output\n",
        "      \n",
        "# https://gist.github.com/t-ae/6e1016cc188104d123676ccef3264981\n",
        "# need to read up on pixelshufflers\n",
        "class _PixelShuffler(nn.Module):\n",
        "    def forward(self, input):\n",
        "        batch_size, c, h, w = input.size()\n",
        "        rh, rw = (2, 2)\n",
        "        oh, ow = h * rh, w * rw\n",
        "        oc = c // (rh * rw)\n",
        "        out = input.view(batch_size, rh, rw, oc, h, w)\n",
        "        out = out.permute(0, 3, 4, 1, 5, 2).contiguous()\n",
        "        out = out.view(batch_size, oc, oh, ow)  # channel first\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "\n",
        "        \"\"\"\n",
        "        self.encoder = nn.Sequential(\n",
        "            _ConvLayer(3, 128),\n",
        "            _ConvLayer(128, 256),\n",
        "            _ConvLayer(256, 512),\n",
        "            _ConvLayer(512, 1024),\n",
        "            Flatten(),\n",
        "            nn.Linear(1024 * 4 * 4, 1024),\n",
        "            nn.Linear(1024, 1024 * 4 * 4),\n",
        "            Reshape(),\n",
        "            _UpScale(1024, 512),\n",
        "        )\n",
        "        \"\"\"\n",
        "        \n",
        "        self.encoder = nn.Sequential(\n",
        "            Conv2d(3, 128,kernel_size=5, stride=2),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            Conv2d(128, 256,kernel_size=5, stride=2),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            Conv2d(256, 512,kernel_size=5, stride=2),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            Conv2d(512, 1024,kernel_size=5, stride=2),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            Flatten(),\n",
        "            nn.Linear(1024 * 4 * 4, 1024),\n",
        "            nn.Linear(1024, 1024 * 4 * 4),\n",
        "            Reshape(),\n",
        "            Conv2d(1024, 512 * 4,kernel_size=3),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            _PixelShuffler())\n",
        "\n",
        "        self.decoder_A = nn.Sequential(\n",
        "            Conv2d(512, 256 * 4,kernel_size=3),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            _PixelShuffler(),\n",
        "            Conv2d(256, 128 * 4,kernel_size=3),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            _PixelShuffler(),\n",
        "            Conv2d(128, 64 * 4,kernel_size=3),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            _PixelShuffler(),\n",
        "            Conv2d(64, 3, kernel_size=5, padding=1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "        self.decoder_B = nn.Sequential(\n",
        "            Conv2d(512, 256 * 4,kernel_size=3),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            _PixelShuffler(),\n",
        "            Conv2d(256, 128 * 4,kernel_size=3),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            _PixelShuffler(),\n",
        "            Conv2d(128, 64 * 4,kernel_size=3),\n",
        "            nn.LeakyReLU(0.1, inplace=True),\n",
        "            _PixelShuffler(),\n",
        "            Conv2d(64, 3, kernel_size=5, padding=1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, select='A'):\n",
        "        if select == 'A':\n",
        "            x = self.encoder(x)\n",
        "            x = self.decoder_A(x)\n",
        "        else:\n",
        "            x = self.encoder(x)\n",
        "            x = self.decoder_B(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KPzDAqw03G6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unn-mDpi1PtS",
        "colab_type": "code",
        "outputId": "6a8ba279-84b9-4091-8863-15a39aaee057",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  print('The GPU is being used to train the model')\n",
        "  device = torch.device('cuda:0')\n",
        "  cudnn.benchmark = True\n",
        "else:\n",
        "    print('The CPU is being used to train the model. This will be slower.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The GPU is being used to train the model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nO6nhzW2SWF",
        "colab_type": "code",
        "outputId": "a206d76d-0e66-4457-941a-4d83b6afb7a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print('The datasets are being loaded into RAM... \\n Note: This might take some time...')\n",
        "\n",
        "images_Tobey = get_image_paths(\"./fiveclips.mp4_faces\")\n",
        "images_Holland = get_image_paths(\"./holland1.mp4_faces\")\n",
        "images_Tobey = normalize_images(load_images(images_Tobey))\n",
        "images_Holland = normalize_images(load_images(images_Holland))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The datasets are being loaded into RAM... \n",
            " Note: This might take some time...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh7yH6gE239B",
        "colab_type": "code",
        "outputId": "dff29e27-3db1-420e-cb52-7d3ab40ff505",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        }
      },
      "source": [
        "model = Autoencoder().to(device)\n",
        "\n",
        "criterion = nn.L1Loss()\n",
        "optimizer_1 = optim.Adam([{'params': model.encoder.parameters()},\n",
        "                          {'params': model.decoder_A.parameters()}]\n",
        "                         , lr=5e-5, betas=(0.5, 0.999))\n",
        "optimizer_2 = optim.Adam([{'params': model.encoder.parameters()},\n",
        "                          {'params': model.decoder_B.parameters()}]\n",
        "                         , lr=5e-5, betas=(0.5, 0.999))\n",
        "batchSize = 64\n",
        "endPoint = 100001\n",
        "whenToSave = 100\n",
        "\n",
        "\n",
        "### we check whether a new model is being trained or whether \n",
        "### we are continuing to train the same model \n",
        "if os.path.isdir('checkpoint'):\n",
        "    try:\n",
        "        checkpoint = torch.load('./checkpoint/autoencoder.t7')\n",
        "        model.load_state_dict(checkpoint['state'])\n",
        "        startPoint = checkpoint['epoch']\n",
        "    except FileNotFoundError:\n",
        "        pass\n",
        "else:\n",
        "    startPoint = 0\n",
        "\n",
        "count = 0\n",
        "for epoch in range(start_epoch, end_epoch):\n",
        "  loopBatchSize = batchSize\n",
        "  \n",
        "  if count == 1:\n",
        "    break\n",
        "  \n",
        "  \n",
        "  warped_B, target_B = generateTrainingData(images_Holland, loopBatchSize)\n",
        "  warped_A, target_A = generateTrainingData(images_Tobey, loopBatchSize)\n",
        "  warped_B = toTensor(warped_B)\n",
        "  warped_A = toTensor(warped_A)\n",
        "  target_B = toTensor(target_B)\n",
        "  target_A = toTensor(target_A)\n",
        "  \n",
        "  warped_A = warped_A.to(device).float()\n",
        "  target_A = target_A.to(device).float()\n",
        "  warped_B = warped_B.to(device).float()\n",
        "  target_B = target_B.to(device).float()\n",
        "  \n",
        "  optimizer_1.zero_grad()\n",
        "  optimizer_2.zero_grad()\n",
        "    \n",
        "  warped_A = model(warped_A, 'A')\n",
        "  warped_B = model(warped_B, 'B')\n",
        "        \n",
        "  loss1 = criterion(warped_A, target_A)\n",
        "  loss2 = criterion(warped_B, target_B)\n",
        "  loss = loss1.item() + loss2.item()\n",
        "  loss1.backward()\n",
        "  loss2.backward()\n",
        "  optimizer_1.step()\n",
        "  optimizer_2.step()\n",
        "  \n",
        "  print('epoch: {}, lossTobey:{}, lossHolland:{}'.format(epoch, loss1.item(), loss2.item()))\n",
        "  \n",
        "  count += 1\n",
        "\n",
        "  if epoch % whenToSave == 0:\n",
        "    print('===> Saving models...')\n",
        "    state = {\n",
        "        'state': model.state_dict(),'epoch': epoch}\n",
        "    if not os.path.isdir('checkpoint'):\n",
        "      os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint/autoencoder.t7')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-c9f5dd8579f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./checkpoint/autoencoder.t7'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mstartPoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 839\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    840\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Autoencoder:\n\tMissing key(s) in state_dict: \"encoder.0.weight\", \"encoder.0.bias\", \"encoder.2.weight\", \"encoder.2.bias\", \"encoder.4.weight\", \"encoder.4.bias\", \"encoder.9.weight\", \"encoder.9.bias\", \"encoder.10.weight\", \"encoder.10.bias\", \"encoder.12.weight\", \"encoder.12.bias\", \"decoder_A.0.weight\", \"decoder_A.0.bias\", \"decoder_A.6.weight\", \"decoder_A.6.bias\", \"decoder_A.9.weight\", \"decoder_A.9.bias\", \"decoder_B.0.weight\", \"decoder_B.0.bias\", \"decoder_B.6.weight\", \"decoder_B.6.bias\", \"decoder_B.9.weight\", \"decoder_B.9.bias\". \n\tUnexpected key(s) in state_dict: \"encoder.0.conv2.weight\", \"encoder.0.conv2.bias\", \"encoder.1.conv2.weight\", \"encoder.1.conv2.bias\", \"encoder.2.conv2.weight\", \"encoder.2.conv2.bias\", \"encoder.3.conv2.weight\", \"encoder.3.conv2.bias\", \"encoder.5.weight\", \"encoder.5.bias\", \"encoder.8.conv2_.weight\", \"encoder.8.conv2_.bias\", \"decoder_A.0.conv2_.weight\", \"decoder_A.0.conv2_.bias\", \"decoder_A.1.conv2_.weight\", \"decoder_A.1.conv2_.bias\", \"decoder_A.2.conv2_.weight\", \"decoder_A.2.conv2_.bias\", \"decoder_B.0.conv2_.weight\", \"decoder_B.0.conv2_.bias\", \"decoder_B.1.conv2_.weight\", \"decoder_B.1.conv2_.bias\", \"decoder_B.2.conv2_.weight\", \"decoder_B.2.conv2_.bias\". \n\tsize mismatch for encoder.6.weight: copying a param with shape torch.Size([16384, 1024]) from checkpoint, the shape in current model is torch.Size([1024, 512, 5, 5]).\n\tsize mismatch for encoder.6.bias: copying a param with shape torch.Size([16384]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for decoder_A.3.weight: copying a param with shape torch.Size([3, 64, 5, 5]) from checkpoint, the shape in current model is torch.Size([512, 256, 3, 3]).\n\tsize mismatch for decoder_A.3.bias: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder_B.3.weight: copying a param with shape torch.Size([3, 64, 5, 5]) from checkpoint, the shape in current model is torch.Size([512, 256, 3, 3]).\n\tsize mismatch for decoder_B.3.bias: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([512])."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnowbCy6Hq4c",
        "colab_type": "code",
        "outputId": "fdd7fb72-36a2-4dc4-a2cf-c3083e392ef2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/spidermen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0762d5_UGkiu",
        "colab_type": "code",
        "outputId": "f888f48b-4d29-478a-f92a-d5818bbc98e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('===> Saving final model')\n",
        "state = {'state': model.state_dict(),'epoch': epoch}\n",
        "if not os.path.isdir('checkpoint'):\n",
        "  os.mkdir('checkpoint')\n",
        "torch.save(state, './checkpoint/autoencoder.t7')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===> Saving final model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERKLmaZ2XjiD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca7C-XZ5FrWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOdBIKx9F7UJ",
        "colab_type": "code",
        "outputId": "810892f2-381f-4fb4-fcdf-a3904ff2180e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e77bp3OF8UH",
        "colab_type": "code",
        "outputId": "fc16c787-d043-4044-9344-d59adf05a490",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "y = torch.torch.Tensor"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-ae298ce1defd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: ones_like() received an invalid combination of arguments - got (list), but expected one of:\n * (Tensor input, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Tensor input, bool requires_grad)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mFXaK1KGGbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}